# MVP Engenharia de Dados - PUC

## ğŸ“Š VisÃ£o Geral

Este projeto implementa um pipeline ETL (Extract, Transform, Load) completo para o MVP da disciplina de Engenharia de Dados da PUC. O sistema Ã© modular, escalÃ¡vel e segue as melhores prÃ¡ticas de engenharia de dados.

## ğŸ¯ Objetivos

- Implementar um pipeline ETL robusto e modular
- Demonstrar processos de extraÃ§Ã£o, transformaÃ§Ã£o e carga de dados
- Aplicar boas prÃ¡ticas de engenharia de dados
- Fornecer documentaÃ§Ã£o clara e exemplos prÃ¡ticos

## ğŸ—ï¸ Arquitetura

O projeto estÃ¡ organizado em trÃªs mÃ³dulos principais:

```
mvpPucEngenhariaDados/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract/          # MÃ³dulo de extraÃ§Ã£o de dados
â”‚   â”œâ”€â”€ transform/        # MÃ³dulo de transformaÃ§Ã£o de dados
â”‚   â”œâ”€â”€ load/            # MÃ³dulo de carga de dados
â”‚   â”œâ”€â”€ pipeline.py      # Orquestrador do pipeline ETL
â”‚   â””â”€â”€ generate_sample_data.py  # Gerador de dados de exemplo
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/             # Dados brutos
â”‚   â””â”€â”€ processed/       # Dados processados
â”œâ”€â”€ config/              # Arquivos de configuraÃ§Ã£o
â”œâ”€â”€ logs/                # Logs do sistema
â”œâ”€â”€ requirements.txt     # DependÃªncias Python
â”œâ”€â”€ Dockerfile          # ConfiguraÃ§Ã£o Docker
â””â”€â”€ docker-compose.yml  # OrquestraÃ§Ã£o de containers
```

## ğŸ”§ Componentes do Pipeline

### 1. Extract (ExtraÃ§Ã£o)
- ExtraÃ§Ã£o de dados de mÃºltiplas fontes (CSV, JSON, APIs)
- Suporte a diferentes formatos de dados
- ValidaÃ§Ã£o e logging de dados extraÃ­dos

### 2. Transform (TransformaÃ§Ã£o)
- RemoÃ§Ã£o de duplicatas
- Tratamento de valores faltantes
- NormalizaÃ§Ã£o de dados
- AgregaÃ§Ãµes e filtros
- ConversÃ£o de tipos de dados

### 3. Load (Carga)
- Carga para CSV, JSON, Parquet
- Suporte a bancos de dados relacionais
- Diferentes modos de carga (append, replace)

## ğŸš€ Como Executar

### PrÃ©-requisitos

- Python 3.11+
- Docker e Docker Compose (opcional)
- PostgreSQL (opcional, para persistÃªncia em banco de dados)

### InstalaÃ§Ã£o Local

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/ricferal/mvpPucEngenhariaDados.git
cd mvpPucEngenhariaDados
```

2. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

3. Gere dados de exemplo:
```bash
python src/generate_sample_data.py
```

4. Execute o pipeline:
```bash
python src/pipeline.py
```

### ExecuÃ§Ã£o com Docker

1. Build e execute os containers:
```bash
docker-compose up --build
```

2. Para executar em background:
```bash
docker-compose up -d
```

3. Para parar os containers:
```bash
docker-compose down
```

## ğŸ“ Exemplo de Uso

### Uso BÃ¡sico do Pipeline

```python
from src.pipeline import ETLPipeline

# Inicializar o pipeline
pipeline = ETLPipeline(config_path="config/pipeline_config.yaml")

# Executar o pipeline
pipeline.run_pipeline(
    source_path="data/raw/sample_data.csv",
    output_path="data/processed/processed_data.csv"
)
```

### Uso Individual dos MÃ³dulos

```python
from src.extract.extractor import DataExtractor
from src.transform.transformer import DataTransformer
from src.load.loader import DataLoader

# ExtraÃ§Ã£o
extractor = DataExtractor()
df = extractor.extract_from_csv("data/raw/sample_data.csv")

# TransformaÃ§Ã£o
transformer = DataTransformer()
df = transformer.remove_duplicates(df)
df = transformer.handle_missing_values(df, strategy='drop')

# Carga
loader = DataLoader()
loader.load_to_csv(df, "data/processed/output.csv")
```

## ğŸ“Š Dados de Exemplo

O projeto inclui um gerador de dados de vendas com as seguintes caracterÃ­sticas:

- **1000+ registros** de transaÃ§Ãµes de vendas
- **MÃºltiplas categorias** de produtos
- **Dados de clientes** e regiÃµes
- **Duplicatas intencionais** (~5%) para demonstrar limpeza
- **Valores faltantes** (~3%) para demonstrar tratamento

## ğŸ” Funcionalidades

### ExtraÃ§Ã£o
- âœ… Leitura de CSV
- âœ… Leitura de JSON
- âœ… IntegraÃ§Ã£o com APIs REST
- âœ… ValidaÃ§Ã£o de dados na entrada

### TransformaÃ§Ã£o
- âœ… RemoÃ§Ã£o de duplicatas
- âœ… Tratamento de valores nulos
- âœ… NormalizaÃ§Ã£o de dados numÃ©ricos
- âœ… AgregaÃ§Ãµes e agrupamentos
- âœ… Filtros e condiÃ§Ãµes
- âœ… ConversÃ£o de tipos

### Carga
- âœ… ExportaÃ§Ã£o para CSV
- âœ… ExportaÃ§Ã£o para JSON
- âœ… ExportaÃ§Ã£o para Parquet
- âœ… Carga em banco de dados SQL
- âœ… Suporte a append e replace

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python 3.11**: Linguagem principal
- **Pandas**: ManipulaÃ§Ã£o de dados
- **NumPy**: OperaÃ§Ãµes numÃ©ricas
- **SQLAlchemy**: ORM para bancos de dados
- **PostgreSQL**: Banco de dados relacional
- **Docker**: ContainerizaÃ§Ã£o
- **PyYAML**: ConfiguraÃ§Ã£o
- **Requests**: IntegraÃ§Ã£o com APIs

## ğŸ“ˆ Monitoramento e Logs

O sistema gera logs detalhados de todas as operaÃ§Ãµes:

```
2024-XX-XX XX:XX:XX - pipeline - INFO - Starting ETL Pipeline
2024-XX-XX XX:XX:XX - extractor - INFO - Extracted 1000 rows from CSV
2024-XX-XX XX:XX:XX - transformer - INFO - Removed 50 duplicate rows
2024-XX-XX XX:XX:XX - loader - INFO - Successfully loaded 950 rows
2024-XX-XX XX:XX:XX - pipeline - INFO - Pipeline completed in 2.34 seconds
```

## ğŸ§ª Testes

Para executar os mÃ³dulos individualmente:

```bash
# Testar extrator
python -m src.extract.extractor

# Testar transformador
python -m src.transform.transformer

# Testar loader
python -m src.load.loader
```

## ğŸ“š DocumentaÃ§Ã£o Adicional

- **ConfiguraÃ§Ã£o**: Veja `config/pipeline_config.yaml` para opÃ§Ãµes de configuraÃ§Ã£o
- **Exemplos**: Execute `src/generate_sample_data.py` para gerar dados de teste
- **Logs**: Verifique o diretÃ³rio `logs/` para histÃ³rico de execuÃ§Ãµes

## ğŸ¤ ContribuiÃ§Ãµes

Este Ã© um projeto acadÃªmico para o MVP de Engenharia de Dados da PUC.

## ğŸ“„ LicenÃ§a

Projeto acadÃªmico - PUC

## ğŸ‘¨â€ğŸ’» Autor

Desenvolvido como MVP para a disciplina de Engenharia de Dados - PUC

---

## ğŸ“ Aspectos AcadÃªmicos

Este projeto demonstra:

1. **Conceitos de ETL**: ImplementaÃ§Ã£o completa de pipeline Extract-Transform-Load
2. **Qualidade de Dados**: Tratamento de duplicatas, valores faltantes e validaÃ§Ãµes
3. **Modularidade**: CÃ³digo organizado em mÃ³dulos reutilizÃ¡veis
4. **Escalabilidade**: Uso de Docker para facilitar deployment
5. **Boas PrÃ¡ticas**: Logging, tratamento de erros, documentaÃ§Ã£o
6. **IntegraÃ§Ã£o**: Suporte a mÃºltiplas fontes e destinos de dados