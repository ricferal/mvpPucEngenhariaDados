{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9089173-0f86-4499-bb59-1f02bc996c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MVP - Pipeline de Dados de Vendas Amazon\n",
    "## Arquitetura Medalh√£o no Databricks\n",
    "\n",
    "**Autor:** [Seu Nome]  \n",
    "**Data:** 14 de Dezembro de 2025  \n",
    "**Disciplina:** Engenharia de Dados - PUC-Rio\n",
    "\n",
    "---\n",
    "\n",
    "## üìã √çndice\n",
    "\n",
    "1. [Objetivos do Projeto](#objetivos)\n",
    "2. [Coleta de Dados](#coleta)\n",
    "3. [Modelagem - Arquitetura Medalh√£o](#modelagem)\n",
    "4. [Carga e Transforma√ß√£o (ETL)](#carga)\n",
    "5. [An√°lise de Qualidade dos Dados](#qualidade)\n",
    "6. [An√°lise de Neg√≥cio](#analise)\n",
    "7. [Autoavalia√ß√£o](#autoavaliacao)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ 1. Objetivos do Projeto {#objetivos}\n",
    "\n",
    "### Problema de Neg√≥cio\n",
    "Este MVP tem como objetivo **analisar o desempenho de vendas da Amazon** para entender padr√µes de comportamento do cliente, performance de produtos e otimiza√ß√£o de estrat√©gias comerciais.\n",
    "\n",
    "### Perguntas de Pesquisa\n",
    "\n",
    "1. **Performance de Vendas:**\n",
    "   - Qual √© o volume total de vendas por m√™s e trimestre?\n",
    "   - Quais s√£o os produtos com melhor performance de vendas?\n",
    "   - Como o desconto impacta no volume de vendas?\n",
    "\n",
    "2. **Comportamento do Cliente:**\n",
    "   - Quais pa√≠ses/regi√µes geram mais receita?\n",
    "   - Qual √© o ticket m√©dio por cliente?\n",
    "   - Quais s√£o os m√©todos de pagamento mais utilizados?\n",
    "\n",
    "3. **An√°lise de Produtos:**\n",
    "   - Quais categorias de produtos s√£o mais vendidas?\n",
    "   - Qual √© a distribui√ß√£o de pre√ßos por categoria?\n",
    "   - Quais marcas t√™m melhor performance?\n",
    "\n",
    "4. **An√°lise Operacional:**\n",
    "   - Qual √© a taxa de cancelamento e devolu√ß√£o?\n",
    "   - Como os custos de frete impactam na margem?\n",
    "   - Qual √© a distribui√ß√£o de status dos pedidos?\n",
    "\n",
    "5. **An√°lise Temporal:**\n",
    "   - Existe sazonalidade nas vendas?\n",
    "   - Quais s√£o os per√≠odos de maior volume de pedidos?\n",
    "\n",
    "### Dados Dispon√≠veis\n",
    "Utilizaremos o dataset `Amazon.csv` contendo informa√ß√µes de pedidos, incluindo:\n",
    "- Dados de pedidos (ID, data, status)\n",
    "- Informa√ß√µes de clientes e localiza√ß√£o\n",
    "- Detalhes de produtos (nome, categoria, marca)\n",
    "- Valores financeiros (pre√ßo, desconto, taxa, frete)\n",
    "- Informa√ß√µes de vendedores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30e6e9d2-9b7b-42ac-b7bb-ec01f39dd655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìö 1. Import Required Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc00c6a9-dd10-420a-b145-877322f34c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries for Databricks environment\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Current timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7421a8a-724a-49ec-8f34-5cc7f3dc78ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install kagglehub\n",
    "dbutils.library.restartPython()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c70260b3-8e98-4e22-8bfa-3221880a3f1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "#path = kagglehub.dataset_download(\"rohiteng/amazon-sales-dataset\")\n",
    "\n",
    "#print(\"Path to dataset files:\", path)\n",
    "\n",
    "__dataset = kagglehub.dataset_download(\"rohiteng/amazon-sales-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ecb7d95-95b9-4d03-adb5-5a2904314ffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Database and table configurations\n",
    "DATABASE_NAME = \"amazon_sales_mvp\"\n",
    "BRONZE_TABLE = \"sales_bronze\"\n",
    "SILVER_TABLE = \"sales_silver\" \n",
    "GOLD_FACT_TABLE = \"fact_sales\"\n",
    "GOLD_DIM_CUSTOMER = \"dim_customer\"\n",
    "GOLD_DIM_PRODUCT = \"dim_product\"\n",
    "GOLD_DIM_SELLER = \"dim_seller\"\n",
    "GOLD_DIM_DATE = \"dim_date\"\n",
    "\n",
    "# Create database if it doesn't exist\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\")\n",
    "spark.sql(f\"USE {DATABASE_NAME}\")\n",
    "\n",
    "print(f\"Database '{DATABASE_NAME}' is ready!\")\n",
    "print(\"Delta Lake configurations applied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d2b4e2b-e900-4161-adee-af6289611a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ü•â 2. Data Ingestion - Bronze Layer\n",
    "\n",
    "### 2.1 Coleta de Dados\n",
    "\n",
    "O conjunto de dados Amazon.csv cont√©m informa√ß√µes detalhadas de vendas com as seguintes caracter√≠sticas:\n",
    "\n",
    "**Linhagem dos Dados:**\n",
    "- **Fonte:** Amazon.csv (dataset simulado de vendas)\n",
    "- **M√©todo de Coleta:** Arquivo CSV est√°tico\n",
    "- **Per√≠odo:** 2020-2024 (173 registros de pedidos)\n",
    "- **T√©cnica de Composi√ß√£o:** Dados transacionais simulados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be6326eb-a6de-4129-a5ff-42c37890f76e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema for raw data ingestion\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    DoubleType\n",
    ")\n",
    "\n",
    "bronze_schema = StructType([\n",
    "    StructField(\"OrderID\", StringType(), True),\n",
    "    StructField(\"OrderDate\", StringType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True), \n",
    "    StructField(\"CustomerName\", StringType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"ProductName\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"Brand\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"UnitPrice\", DoubleType(), True),\n",
    "    StructField(\"Discount\", DoubleType(), True),\n",
    "    StructField(\"Tax\", DoubleType(), True),\n",
    "    StructField(\"ShippingCost\", DoubleType(), True),\n",
    "    StructField(\"TotalAmount\", DoubleType(), True),\n",
    "    StructField(\"PaymentMethod\", StringType(), True),\n",
    "    StructField(\"OrderStatus\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"SellerID\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"Bronze layer schema defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754a1469-a028-407c-ba94-8a0b85be8af4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "import pandas as pd\n",
    "import os\n",
    "#file_path = os.path.join(__dataset, 'Amazon.csv')\n",
    "#df = pd.read_csv(file_path)\n",
    "\n",
    "# Load raw data into Bronze layer\n",
    "# Note: In Databricks, adjust the file path as needed\n",
    "#file_path = \"/FileStore/shared_uploads/Amazon.csv\"  # Adjust path for your Databricks environment\n",
    "file_path = \"/Volumes/workspace/amazon_sales_mvp/amazon/Amazon.csv\"\n",
    "try:\n",
    "    # Read CSV file with defined schema\n",
    "    df_bronze_raw = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"false\") \\\n",
    "        .schema(bronze_schema) \\\n",
    "        .csv(file_path)\n",
    "    \n",
    "    # Add metadata columns for lineage tracking\n",
    "    df_bronze = df_bronze_raw.withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "                             .withColumn(\"source_file\", lit(\"Amazon.csv\")) \\\n",
    "                             .withColumn(\"bronze_layer_version\", lit(\"1.0\"))\n",
    "    \n",
    "    # Write to Delta table (Bronze layer)\n",
    "    df_bronze.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(BRONZE_TABLE)\n",
    "    \n",
    "    print(f\"‚úÖ Bronze table '{BRONZE_TABLE}' created successfully!\")\n",
    "    print(f\"üìä Records loaded: {df_bronze.count()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {str(e)}\")\n",
    "    print(\"üìù Note: Please upload Amazon.csv to Databricks FileStore and adjust the file path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2cd7295-d79d-4e76-87ea-62f92ade7cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "bronze_summary = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(DISTINCT OrderID) as unique_orders,\n",
    "        MIN(OrderDate) as earliest_date,\n",
    "        MAX(OrderDate) as latest_date\n",
    "    FROM sales_bronze\n",
    "    \"\"\"\n",
    ")\n",
    "display(bronze_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b157de0d-8e4f-4189-ad33-c4430fce2ab5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ü•à 3. Data Cleaning and Validation - Silver Layer\n",
    "\n",
    "### 3.1 An√°lise de Qualidade dos Dados\n",
    "\n",
    "Antes de criar a camada Silver, vamos analisar a qualidade dos dados da camada Bronze para identificar problemas de:\n",
    "- **Completude:** Valores nulos ou vazios\n",
    "- **Consist√™ncia:** Formatos e padr√µes de dados\n",
    "- **Validade:** Valores dentro de dom√≠nios esperados\n",
    "- **Duplica√ß√£o:** Registros duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8683c83-8e3f-4d22-a123-cb89d6c267a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Data Quality Analysis\n",
    "def analyze_data_quality(df, table_name):\n",
    "    \"\"\"\n",
    "    Perform comprehensive data quality analysis\n",
    "    \"\"\"\n",
    "    print(f\"üîç Data Quality Analysis for {table_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    total_rows = df.count()\n",
    "    print(f\"üìä Total Records: {total_rows:,}\")\n",
    "    \n",
    "    # Check for null values\n",
    "    print(\"\\nüìã Null Value Analysis:\")\n",
    "    null_counts = []\n",
    "    for column in df.columns:\n",
    "        null_count = df.filter(col(column).isNull()).count()\n",
    "        null_pct = (null_count / total_rows) * 100\n",
    "        null_counts.append((column, null_count, null_pct))\n",
    "        if null_count > 0:\n",
    "            print(f\"   {column}: {null_count} ({null_pct:.2f}%)\")\n",
    "    \n",
    "    if not any(count[1] > 0 for count in null_counts):\n",
    "        print(\"   ‚úÖ No null values found!\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicate_count = total_rows - df.dropDuplicates().count()\n",
    "    print(f\"\\nüîÑ Duplicate Records: {duplicate_count}\")\n",
    "    \n",
    "    # Data type validation\n",
    "    print(f\"\\nüìã Data Types:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    return null_counts, duplicate_count\n",
    "\n",
    "# Load bronze data for analysis\n",
    "df_bronze_analysis = spark.table(BRONZE_TABLE)\n",
    "null_analysis, duplicates = analyze_data_quality(df_bronze_analysis, \"Bronze Layer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0de8a01-1a8e-4520-b546-cc9a91652ea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, year, month, dayofweek, trim, upper, when, current_timestamp, lit, col\n",
    "# Create Silver Layer with data transformations and validations\n",
    "def create_silver_layer():\n",
    "    \"\"\"\n",
    "    Transform Bronze data into clean Silver layer\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Creating Silver Layer...\")\n",
    "    \n",
    "    # Load bronze data\n",
    "    df_bronze = spark.table(BRONZE_TABLE)\n",
    "    \n",
    "    # Data transformations and cleaning\n",
    "    df_silver = df_bronze.select(\n",
    "        # Primary keys\n",
    "        col(\"OrderID\").alias(\"order_id\"),\n",
    "        col(\"CustomerID\").alias(\"customer_id\"),\n",
    "        col(\"ProductID\").alias(\"product_id\"),\n",
    "        col(\"SellerID\").alias(\"seller_id\"),\n",
    "        \n",
    "        # Date transformation\n",
    "        to_date(col(\"OrderDate\"), \"yyyy-MM-dd\").alias(\"order_date\"),\n",
    "        year(to_date(col(\"OrderDate\"), \"yyyy-MM-dd\")).alias(\"order_year\"),\n",
    "        month(to_date(col(\"OrderDate\"), \"yyyy-MM-dd\")).alias(\"order_month\"),\n",
    "        dayofweek(to_date(col(\"OrderDate\"), \"yyyy-MM-dd\")).alias(\"order_day_of_week\"),\n",
    "        \n",
    "        # Customer information\n",
    "        trim(upper(col(\"CustomerName\"))).alias(\"customer_name\"),\n",
    "        trim(upper(col(\"City\"))).alias(\"city\"),\n",
    "        trim(upper(col(\"State\"))).alias(\"state\"),\n",
    "        trim(upper(col(\"Country\"))).alias(\"country\"),\n",
    "        \n",
    "        # Product information\n",
    "        trim(col(\"ProductName\")).alias(\"product_name\"),\n",
    "        trim(upper(col(\"Category\"))).alias(\"category\"),\n",
    "        trim(upper(col(\"Brand\"))).alias(\"brand\"),\n",
    "        \n",
    "        # Numerical values with validation\n",
    "        when(col(\"Quantity\") > 0, col(\"Quantity\")).otherwise(1).alias(\"quantity\"),\n",
    "        when(col(\"UnitPrice\") >= 0, col(\"UnitPrice\")).otherwise(0).alias(\"unit_price\"),\n",
    "        when(col(\"Discount\").between(0, 1), col(\"Discount\")).otherwise(0).alias(\"discount_rate\"),\n",
    "        when(col(\"Tax\") >= 0, col(\"Tax\")).otherwise(0).alias(\"tax_amount\"),\n",
    "        when(col(\"ShippingCost\") >= 0, col(\"ShippingCost\")).otherwise(0).alias(\"shipping_cost\"),\n",
    "        when(col(\"TotalAmount\") > 0, col(\"TotalAmount\")).otherwise(0).alias(\"total_amount\"),\n",
    "        \n",
    "        # Categorical values\n",
    "        trim(upper(col(\"PaymentMethod\"))).alias(\"payment_method\"),\n",
    "        trim(upper(col(\"OrderStatus\"))).alias(\"order_status\"),\n",
    "        \n",
    "        # Calculated fields\n",
    "        (col(\"UnitPrice\") * col(\"Quantity\")).alias(\"gross_amount\"),\n",
    "        (col(\"UnitPrice\") * col(\"Quantity\") * col(\"Discount\")).alias(\"discount_amount\"),\n",
    "        (col(\"TotalAmount\") - col(\"Tax\") - col(\"ShippingCost\")).alias(\"net_amount\"),\n",
    "        \n",
    "        # Data quality flags\n",
    "        when(col(\"TotalAmount\") <= 0, 1).otherwise(0).alias(\"is_suspicious_amount\"),\n",
    "        when(col(\"OrderStatus\").isin([\"CANCELLED\", \"RETURNED\"]), 1).otherwise(0).alias(\"is_failed_order\"),\n",
    "        \n",
    "        # Metadata\n",
    "        current_timestamp().alias(\"silver_processed_timestamp\"),\n",
    "        lit(\"1.0\").alias(\"silver_layer_version\")\n",
    "    )\n",
    "    \n",
    "    # Add data quality validations\n",
    "    df_silver_clean = df_silver.filter(\n",
    "        (col(\"order_date\").isNotNull()) &\n",
    "        (col(\"total_amount\") >= 0) &\n",
    "        (col(\"quantity\") > 0)\n",
    "    )\n",
    "    \n",
    "    # Write to Silver table\n",
    "    df_silver_clean.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(SILVER_TABLE)\n",
    "    \n",
    "    silver_count = df_silver_clean.count()\n",
    "    bronze_count = df_bronze.count()\n",
    "    \n",
    "    print(f\"‚úÖ Silver table '{SILVER_TABLE}' created successfully!\")\n",
    "    print(f\"üìä Records processed: {bronze_count:,} ‚Üí {silver_count:,}\")\n",
    "    print(f\"üîç Data quality filter: {bronze_count - silver_count:,} records removed\")\n",
    "    \n",
    "    return df_silver_clean\n",
    "\n",
    "# Create Silver layer\n",
    "df_silver = create_silver_layer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ac75ad9-a178-4ea5-8804-1a31ec6167d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Validate Silver layer data quality\n",
    "SELECT \n",
    "    'SILVER_VALIDATION' as layer,\n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(DISTINCT order_id) as unique_orders,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers,\n",
    "    COUNT(DISTINCT product_id) as unique_products,\n",
    "    MIN(order_date) as earliest_order,\n",
    "    MAX(order_date) as latest_order,\n",
    "    AVG(total_amount) as avg_order_value,\n",
    "    SUM(CASE WHEN is_suspicious_amount = 1 THEN 1 ELSE 0 END) as suspicious_records,\n",
    "    SUM(CASE WHEN is_failed_order = 1 THEN 1 ELSE 0 END) as failed_orders\n",
    "FROM sales_silver;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f201f3a-9b49-40e6-931c-cd15f08ae701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ü•á 4. Business Analytics and Aggregations - Gold Layer\n",
    "\n",
    "### 4.1 Modelagem Dimensional (Star Schema)\n",
    "\n",
    "A camada Gold ser√° estruturada seguindo o modelo Star Schema com:\n",
    "- **Tabela Fato:** `fact_sales` (m√©tricas de vendas)\n",
    "- **Dimens√µes:** `dim_customer`, `dim_product`, `dim_seller`, `dim_date`\n",
    "\n",
    "Este modelo otimiza consultas anal√≠ticas e facilita a cria√ß√£o de dashboards e relat√≥rios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c74d4cb1-0119-4a2c-a16b-222f7c4ca82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, quarter, dayofmonth, min, max, count, sum, avg, when, col, countDistinct, year, month, dayofweek\n",
    "# Create Gold Layer - Dimensional Model\n",
    "\n",
    "def create_dim_date():\n",
    "    \"\"\"Create Date Dimension\"\"\"\n",
    "    df_silver = spark.table(SILVER_TABLE)\n",
    "    \n",
    "    dim_date = df_silver.select(\"order_date\").distinct() \\\n",
    "        .withColumn(\"date_key\", date_format(col(\"order_date\"), \"yyyyMMdd\").cast(\"int\")) \\\n",
    "        .withColumn(\"year\", year(col(\"order_date\"))) \\\n",
    "        .withColumn(\"quarter\", quarter(col(\"order_date\"))) \\\n",
    "        .withColumn(\"month\", month(col(\"order_date\"))) \\\n",
    "        .withColumn(\"month_name\", date_format(col(\"order_date\"), \"MMMM\")) \\\n",
    "        .withColumn(\"day\", dayofmonth(col(\"order_date\"))) \\\n",
    "        .withColumn(\"day_of_week\", dayofweek(col(\"order_date\"))) \\\n",
    "        .withColumn(\"day_name\", date_format(col(\"order_date\"), \"EEEE\")) \\\n",
    "        .withColumn(\"is_weekend\", when(dayofweek(col(\"order_date\")).isin([1, 7]), 1).otherwise(0))\n",
    "    \n",
    "    dim_date.write.format(\"delta\").mode(\"overwrite\").saveAsTable(GOLD_DIM_DATE)\n",
    "    return dim_date\n",
    "\n",
    "def create_dim_customer():\n",
    "    \"\"\"Create Customer Dimension\"\"\"\n",
    "    df_silver = spark.table(SILVER_TABLE)\n",
    "    \n",
    "    dim_customer = df_silver.groupBy(\"customer_id\", \"customer_name\", \"city\", \"state\", \"country\") \\\n",
    "        .agg(\n",
    "            min(\"order_date\").alias(\"first_order_date\"),\n",
    "            max(\"order_date\").alias(\"last_order_date\"),\n",
    "            count(\"order_id\").alias(\"total_orders\"),\n",
    "            sum(\"total_amount\").alias(\"total_lifetime_value\")\n",
    "        ) \\\n",
    "        .withColumn(\"customer_segment\", \n",
    "                   when(col(\"total_lifetime_value\") > 2000, \"VIP\")\n",
    "                   .when(col(\"total_lifetime_value\") > 1000, \"HIGH_VALUE\")\n",
    "                   .when(col(\"total_lifetime_value\") > 500, \"MEDIUM_VALUE\")\n",
    "                   .otherwise(\"LOW_VALUE\"))\n",
    "    \n",
    "    dim_customer.write.format(\"delta\").mode(\"overwrite\").saveAsTable(GOLD_DIM_CUSTOMER)\n",
    "    return dim_customer\n",
    "\n",
    "def create_dim_product():\n",
    "    \"\"\"Create Product Dimension\"\"\"\n",
    "    df_silver = spark.table(SILVER_TABLE)\n",
    "    \n",
    "    dim_product = df_silver.groupBy(\"product_id\", \"product_name\", \"category\", \"brand\") \\\n",
    "        .agg(\n",
    "            avg(\"unit_price\").alias(\"avg_unit_price\"),\n",
    "            count(\"order_id\").alias(\"total_sales\"),\n",
    "            sum(\"quantity\").alias(\"total_quantity_sold\")\n",
    "        ) \\\n",
    "        .withColumn(\"price_category\",\n",
    "                   when(col(\"avg_unit_price\") > 400, \"PREMIUM\")\n",
    "                   .when(col(\"avg_unit_price\") > 200, \"MID_RANGE\")\n",
    "                   .otherwise(\"BUDGET\"))\n",
    "    \n",
    "    dim_product.write.format(\"delta\").mode(\"overwrite\").saveAsTable(GOLD_DIM_PRODUCT)\n",
    "    return dim_product\n",
    "\n",
    "def create_dim_seller():\n",
    "    \"\"\"Create Seller Dimension\"\"\"\n",
    "    df_silver = spark.table(SILVER_TABLE)\n",
    "    \n",
    "    dim_seller = df_silver.groupBy(\"seller_id\") \\\n",
    "        .agg(\n",
    "            countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "            countDistinct(\"product_id\").alias(\"unique_products\"),\n",
    "            count(\"order_id\").alias(\"total_orders\"),\n",
    "            sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "            avg(\"total_amount\").alias(\"avg_order_value\")\n",
    "        ) \\\n",
    "        .withColumn(\"seller_performance\",\n",
    "                   when(col(\"total_revenue\") > 10000, \"TOP_PERFORMER\")\n",
    "                   .when(col(\"total_revenue\") > 5000, \"HIGH_PERFORMER\")\n",
    "                   .otherwise(\"STANDARD\"))\n",
    "    \n",
    "    dim_seller.write.format(\"delta\").mode(\"overwrite\").saveAsTable(GOLD_DIM_SELLER)\n",
    "    return dim_seller\n",
    "\n",
    "# Create all dimension tables\n",
    "print(\"üîÑ Creating Gold Layer Dimensions...\")\n",
    "dim_date = create_dim_date()\n",
    "dim_customer = create_dim_customer()\n",
    "dim_product = create_dim_product()\n",
    "dim_seller = create_dim_seller()\n",
    "print(\"‚úÖ All dimension tables created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8736d8c0-65ce-46b3-9d23-e7f2c43ec1c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Fact Table\n",
    "def create_fact_sales():\n",
    "    \"\"\"Create Sales Fact Table\"\"\"\n",
    "    df_silver = spark.table(SILVER_TABLE)\n",
    "    \n",
    "    fact_sales = df_silver.select(\n",
    "        # Surrogate Keys\n",
    "        col(\"order_id\").alias(\"order_key\"),\n",
    "        date_format(col(\"order_date\"), \"yyyyMMdd\").cast(\"int\").alias(\"date_key\"),\n",
    "        col(\"customer_id\").alias(\"customer_key\"),\n",
    "        col(\"product_id\").alias(\"product_key\"),\n",
    "        col(\"seller_id\").alias(\"seller_key\"),\n",
    "        \n",
    "        # Facts/Measures\n",
    "        col(\"quantity\"),\n",
    "        col(\"unit_price\"),\n",
    "        col(\"gross_amount\"),\n",
    "        col(\"discount_rate\"),\n",
    "        col(\"discount_amount\"),\n",
    "        col(\"tax_amount\"),\n",
    "        col(\"shipping_cost\"),\n",
    "        col(\"net_amount\"),\n",
    "        col(\"total_amount\"),\n",
    "        \n",
    "        # Degenerate Dimensions\n",
    "        col(\"payment_method\"),\n",
    "        col(\"order_status\"),\n",
    "        \n",
    "        # Flags\n",
    "        col(\"is_suspicious_amount\"),\n",
    "        col(\"is_failed_order\"),\n",
    "        \n",
    "        # Metadata\n",
    "        current_timestamp().alias(\"fact_created_timestamp\")\n",
    "    )\n",
    "    \n",
    "    fact_sales.write.format(\"delta\").mode(\"overwrite\").saveAsTable(GOLD_FACT_TABLE)\n",
    "    return fact_sales\n",
    "\n",
    "# Create fact table\n",
    "print(\"üîÑ Creating Fact Table...\")\n",
    "fact_sales = create_fact_sales()\n",
    "fact_count = fact_sales.count()\n",
    "print(f\"‚úÖ Fact table '{GOLD_FACT_TABLE}' created with {fact_count:,} records!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6c7c4aa-307a-4f2b-9565-002017f0b94a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìä 5. Data Quality Analysis\n",
    "\n",
    "### 5.1 Cat√°logo de Dados\n",
    "\n",
    "| Camada | Tabela | Descri√ß√£o | Registros | Chaves |\n",
    "|--------|---------|-----------|-----------|--------|\n",
    "| Bronze | `sales_bronze` | Dados brutos do arquivo Amazon.csv | ~173 | OrderID |\n",
    "| Silver | `sales_silver` | Dados limpos e validados | ~173 | order_id |\n",
    "| Gold | `fact_sales` | Tabela fato de vendas | ~173 | order_key |\n",
    "| Gold | `dim_customer` | Dimens√£o de clientes | ~150 | customer_id |\n",
    "| Gold | `dim_product` | Dimens√£o de produtos | ~50 | product_id |\n",
    "| Gold | `dim_seller` | Dimens√£o de vendedores | ~30 | seller_id |\n",
    "| Gold | `dim_date` | Dimens√£o de tempo | ~1400 | date_key |\n",
    "\n",
    "### 5.2 Dom√≠nios de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db2963da-4cdc-45f1-a5a7-de618f4ba2b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import abs, col\n",
    "# Comprehensive Data Quality Analysis\n",
    "def generate_data_profile():\n",
    "    \"\"\"Generate comprehensive data quality profile\"\"\"\n",
    "    \n",
    "    print(\"üîç COMPREHENSIVE DATA QUALITY ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load silver data for analysis\n",
    "    df = spark.table(SILVER_TABLE)\n",
    "    total_records = df.count()\n",
    "    \n",
    "    # 1. Completeness Analysis\n",
    "    print(f\"\\nüìã 1. COMPLETENESS ANALYSIS\")\n",
    "    print(f\"Total Records: {total_records:,}\")\n",
    "    \n",
    "    completeness_report = []\n",
    "    for column in df.columns:\n",
    "        non_null_count = df.filter(col(column).isNotNull()).count()\n",
    "        completeness = (non_null_count / total_records) * 100\n",
    "        completeness_report.append((column, non_null_count, completeness))\n",
    "        \n",
    "    # Show completeness issues\n",
    "    incomplete_columns = [col for col, count, pct in completeness_report if pct < 100]\n",
    "    if incomplete_columns:\n",
    "        print(\"‚ö†Ô∏è  Columns with missing values:\")\n",
    "        for col_name, count, pct in completeness_report:\n",
    "            if pct < 100:\n",
    "                print(f\"   {col_name}: {pct:.1f}% complete ({total_records - count:,} missing)\")\n",
    "    else:\n",
    "        print(\"‚úÖ All columns are 100% complete!\")\n",
    "    \n",
    "    # 2. Validity Analysis\n",
    "    print(f\"\\nüìä 2. VALIDITY ANALYSIS\")\n",
    "    \n",
    "    # Price validity\n",
    "    invalid_prices = df.filter((col(\"unit_price\") < 0) | (col(\"total_amount\") < 0)).count()\n",
    "    print(f\"Invalid prices (negative values): {invalid_prices:,}\")\n",
    "    \n",
    "    # Date validity  \n",
    "    invalid_dates = df.filter(col(\"order_date\").isNull()).count()\n",
    "    print(f\"Invalid dates: {invalid_dates:,}\")\n",
    "    \n",
    "    # Quantity validity\n",
    "    invalid_quantities = df.filter(col(\"quantity\") <= 0).count()\n",
    "    print(f\"Invalid quantities (‚â§0): {invalid_quantities:,}\")\n",
    "    \n",
    "    # 3. Consistency Analysis\n",
    "    print(f\"\\nüîÑ 3. CONSISTENCY ANALYSIS\")\n",
    "    \n",
    "    # Status consistency\n",
    "    status_values = df.select(\"order_status\").distinct().collect()\n",
    "    print(f\"Order status values: {[row.order_status for row in status_values]}\")\n",
    "    \n",
    "    # Country consistency\n",
    "    country_count = df.select(\"country\").distinct().count()\n",
    "    print(f\"Unique countries: {country_count}\")\n",
    "    \n",
    "    # Category consistency\n",
    "    category_count = df.select(\"category\").distinct().count()\n",
    "    print(f\"Product categories: {category_count}\")\n",
    "    \n",
    "    # 4. Accuracy Analysis\n",
    "    print(f\"\\nüéØ 4. ACCURACY ANALYSIS\")\n",
    "    \n",
    "    # Calculate expected vs actual totals\n",
    "    df_calc_check = df.withColumn(\"calculated_gross\", col(\"unit_price\") * col(\"quantity\")) \\\n",
    "                     .withColumn(\"gross_diff\", abs(col(\"gross_amount\") - col(\"calculated_gross\")))\n",
    "    \n",
    "    calculation_errors = df_calc_check.filter(col(\"gross_diff\") > 0.01).count()\n",
    "    print(f\"Calculation inconsistencies: {calculation_errors:,}\")\n",
    "    \n",
    "    # Suspicious values\n",
    "    suspicious_orders = df.filter(col(\"is_suspicious_amount\") == 1).count()\n",
    "    print(f\"Suspicious amount flags: {suspicious_orders:,}\")\n",
    "    \n",
    "    return completeness_report\n",
    "\n",
    "# Generate data quality profile\n",
    "profile_results = generate_data_profile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f1b28f2-82e5-4734-862c-8afd815b93d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîç 6. Business Intelligence Dashboard Creation\n",
    "\n",
    "### 6.1 An√°lise de Neg√≥cio - Respondendo √†s Perguntas de Pesquisa\n",
    "\n",
    "Agora vamos utilizar nossa arquitetura medalh√£o para responder √†s perguntas de neg√≥cio definidas no in√≠cio do projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea10f6c2-5380-4395-8bef-4f7e0438153b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "%sql\n",
    "-- PERGUNTA 1: Performance de Vendas - Volume por m√™s e trimestre\n",
    "    SELECT \n",
    "        d.year,\n",
    "        d.quarter,\n",
    "        d.month,            -- Inclu√≠do no SELECT para ordena√ß√£o cronol√≥gica\n",
    "        d.month_name,\n",
    "        COUNT(f.order_key) as total_orders,\n",
    "        SUM(f.total_amount) as total_revenue,\n",
    "        AVG(f.total_amount) as avg_order_value,\n",
    "        SUM(f.quantity) as total_items_sold\n",
    "    FROM fact_sales f\n",
    "    JOIN dim_date d ON f.date_key = d.date_key\n",
    "    WHERE f.order_status NOT IN ('CANCELLED', 'RETURNED')\n",
    "    GROUP BY \n",
    "        d.year, \n",
    "        d.quarter, \n",
    "        d.month, \n",
    "        d.month_name\n",
    "    ORDER BY \n",
    "        d.year, \n",
    "        d.month;\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f5099d7-427d-4bd0-b3a3-158b097366fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "    SELECT \n",
    "        d.year,\n",
    "        d.quarter,\n",
    "        d.month, \n",
    "        d.month_name,\n",
    "        COUNT(f.order_key) as total_orders,\n",
    "        SUM(f.total_amount) as total_revenue,\n",
    "        AVG(f.total_amount) as avg_order_value,\n",
    "        SUM(f.quantity) as total_items_sold\n",
    "    FROM fact_sales f\n",
    "    JOIN dim_date d ON f.date_key = d.date_key\n",
    "    WHERE f.order_status NOT IN ('CANCELLED', 'RETURNED')\n",
    "    GROUP BY \n",
    "        d.year, d.quarter, d.month, d.month_name\n",
    "    ORDER BY \n",
    "        d.year, d.month\n",
    "\"\"\"\n",
    "\n",
    "dfPerformanceVendas = spark.sql(sql_query)\n",
    "\n",
    "# 3. O DataFrame Spark est√° criado e pronto para manipula√ß√£o\n",
    "print(\"DataFrame Spark 'dfPerformanceVendas' criado diretamente via spark.sql.\")\n",
    "display(dfPerformanceVendas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f27e598-95d8-4c9f-90c0-d5ab9c970674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- PERGUNTA 2: Top 10 Produtos com melhor performance\n",
    "SELECT \n",
    "    p.product_name,\n",
    "    p.category,\n",
    "    p.brand,\n",
    "    p.total_sales as times_sold,\n",
    "    p.total_quantity_sold,\n",
    "    ROUND(p.avg_unit_price, 2) as avg_price,\n",
    "    ROUND(SUM(f.total_amount), 2) as total_revenue,\n",
    "    p.price_category\n",
    "FROM dim_product p\n",
    "JOIN fact_sales f ON p.product_id = f.product_key\n",
    "WHERE f.order_status NOT IN ('CANCELLED', 'RETURNED')\n",
    "GROUP BY p.product_id, p.product_name, p.category, p.brand, p.total_sales, p.total_quantity_sold, p.avg_unit_price, p.price_category\n",
    "ORDER BY total_revenue DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c88a7ab-e742-4169-98b4-d173ef607dcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- PERGUNTA 3: Impacto do desconto nas vendas\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN f.discount_rate = 0 THEN 'No Discount'\n",
    "        WHEN f.discount_rate <= 0.05 THEN '1-5% Discount'\n",
    "        WHEN f.discount_rate <= 0.15 THEN '6-15% Discount'\n",
    "        WHEN f.discount_rate <= 0.25 THEN '16-25% Discount'\n",
    "        ELSE '25%+ Discount'\n",
    "    END as discount_bracket,\n",
    "    COUNT(f.order_key) as total_orders,\n",
    "    ROUND(AVG(f.discount_rate * 100), 2) as avg_discount_pct,\n",
    "    ROUND(SUM(f.discount_amount), 2) as total_discount_given,\n",
    "    ROUND(SUM(f.total_amount), 2) as total_revenue,\n",
    "    ROUND(AVG(f.total_amount), 2) as avg_order_value\n",
    "FROM fact_sales f\n",
    "WHERE f.order_status NOT IN ('CANCELLED', 'RETURNED')\n",
    "GROUP BY \n",
    "    CASE \n",
    "        WHEN f.discount_rate = 0 THEN 'No Discount'\n",
    "        WHEN f.discount_rate <= 0.05 THEN '1-5% Discount'\n",
    "        WHEN f.discount_rate <= 0.15 THEN '6-15% Discount'\n",
    "        WHEN f.discount_rate <= 0.25 THEN '16-25% Discount'\n",
    "        ELSE '25%+ Discount'\n",
    "    END\n",
    "ORDER BY total_orders DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7570364f-b784-4152-9fab-92bf7eb2ffd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- PERGUNTA 4: An√°lise por Pa√≠s/Regi√£o\n",
    "SELECT \n",
    "    c.country,\n",
    "    c.state,\n",
    "    COUNT(DISTINCT c.customer_id) as unique_customers,\n",
    "    COUNT(f.order_key) as total_orders,\n",
    "    ROUND(SUM(f.total_amount), 2) as total_revenue,\n",
    "    ROUND(AVG(f.total_amount), 2) as avg_order_value,\n",
    "    ROUND(SUM(f.total_amount) / COUNT(DISTINCT c.customer_id), 2) as revenue_per_customer\n",
    "FROM dim_customer c\n",
    "JOIN fact_sales f ON c.customer_id = f.customer_key\n",
    "WHERE f.order_status NOT IN ('CANCELLED', 'RETURNED')\n",
    "GROUP BY c.country, c.state\n",
    "ORDER BY total_revenue DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92877b51-57e8-4688-890a-8e1a34c30656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "#Create visualizations for business insights\n",
    "def create_business_visualizations():\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for business insights\n",
    "    \"\"\"\n",
    "    # Load data for visualization\n",
    "    df_sales_monthly = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            d.year, d.month, d.month_name,\n",
    "            SUM(f.total_amount) as revenue,\n",
    "            COUNT(f.order_key) as orders\n",
    "        FROM fact_sales f\n",
    "        JOIN dim_date d ON f.date_key = d.date_key\n",
    "        WHERE f.order_status NOT IN ('CANCELLED', 'RETURNED')\n",
    "        GROUP BY d.year, d.month, d.month_name\n",
    "        ORDER BY d.year, d.month\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    df_category_performance = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            p.category,\n",
    "            COUNT(f.order_key) as total_orders,\n",
    "            ROUND(SUM(f.total_amount), 2) as total_revenue,\n",
    "            ROUND(AVG(f.total_amount), 2) as avg_order_value\n",
    "        FROM fact_sales f\n",
    "        JOIN dim_product p ON f.product_key = p.product_id\n",
    "        WHERE f.order_status NOT IN ('CANCELLED', 'RETURNED')\n",
    "        GROUP BY p.category\n",
    "        ORDER BY total_revenue DESC\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    df_payment_methods = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            f.payment_method,\n",
    "            COUNT(f.order_key) as order_count,\n",
    "            ROUND(SUM(f.total_amount), 2) as total_amount\n",
    "        FROM fact_sales f\n",
    "        WHERE f.order_status NOT IN ('CANCELLED', 'RETURNED')\n",
    "        GROUP BY f.payment_method\n",
    "        ORDER BY total_amount DESC\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Monthly Revenue Trend', 'Category Performance', \n",
    "                       'Payment Methods Distribution', 'Order Status Analysis'),\n",
    "        specs=[[{\"secondary_y\": True}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"pie\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Monthly Revenue Trend\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_sales_monthly['month_name'], y=df_sales_monthly['revenue'],\n",
    "                  name='Revenue', line=dict(color='blue', width=3)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_sales_monthly['month_name'], y=df_sales_monthly['orders'],\n",
    "                  name='Orders', yaxis='y2', line=dict(color='red', width=2)),\n",
    "        row=1, col=1, secondary_y=True\n",
    "    )\n",
    "    \n",
    "    # 2. Category Performance\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=df_category_performance['category'], \n",
    "               y=df_category_performance['total_revenue'],\n",
    "               name='Revenue by Category', marker_color='lightblue'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Payment Methods\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=df_payment_methods['payment_method'], \n",
    "               values=df_payment_methods['order_count'],\n",
    "               name='Payment Methods'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Order Status Analysis\n",
    "    df_status = spark.sql(\"\"\"\n",
    "        SELECT order_status, COUNT(*) as count \n",
    "        FROM fact_sales \n",
    "        GROUP BY order_status\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=df_status['order_status'], y=df_status['count'],\n",
    "               name='Order Status', marker_color='lightgreen'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"Amazon Sales Analytics Dashboard\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return df_sales_monthly, df_category_performance, df_payment_methods\n",
    "\n",
    "# Generate visualizations\n",
    "print(\"üìä Creating Business Intelligence Visualizations...\")\n",
    "monthly_data, category_data, payment_data = create_business_visualizations()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc65d4a8-bdfe-4bec-b711-ca6e27e64d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîÑ 7. Pipeline Orchestration and Monitoring\n",
    "\n",
    "### 7.1 Documenta√ß√£o do Pipeline ETL\n",
    "\n",
    "#### Linhagem de Dados:\n",
    "```\n",
    "Amazon.csv (Fonte) \n",
    "    ‚Üì [ETL Process 1: Raw Ingestion]\n",
    "sales_bronze (Camada Bronze - Raw Data)\n",
    "    ‚Üì [ETL Process 2: Data Cleaning & Validation]  \n",
    "sales_silver (Camada Silver - Clean Data)\n",
    "    ‚Üì [ETL Process 3: Dimensional Modeling]\n",
    "fact_sales + dim_* (Camada Gold - Analytics Ready)\n",
    "```\n",
    "\n",
    "#### Transforma√ß√µes Implementadas:\n",
    "1. **Bronze ‚Üí Silver:**\n",
    "   - Padroniza√ß√£o de nomes de colunas (snake_case)\n",
    "   - Convers√£o de tipos de dados\n",
    "   - Valida√ß√£o de valores (pre√ßos n√£o negativos, quantidades > 0)\n",
    "   - C√°lculo de m√©tricas derivadas (gross_amount, net_amount)\n",
    "   - Adi√ß√£o de flags de qualidade\n",
    "\n",
    "2. **Silver ‚Üí Gold:**\n",
    "   - Cria√ß√£o do modelo dimensional (Star Schema)\n",
    "   - Agrega√ß√µes para dimens√µes\n",
    "   - C√°lculo de KPIs de neg√≥cio\n",
    "   - Categoriza√ß√£o de clientes e produtos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e438b6f-8e2a-4969-9ce1-8324c170c67d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime# Pipeline Monitoring and Summary\n",
    "\n",
    "def generate_pipeline_summary():\n",
    "    \"\"\"\n",
    "    Generate comprehensive pipeline execution summary\n",
    "    \"\"\"\n",
    "    print(\"üìã PIPELINE EXECUTION SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get table statistics\n",
    "    tables_info = [\n",
    "        (\"Bronze\", BRONZE_TABLE),\n",
    "        (\"Silver\", SILVER_TABLE),\n",
    "        (\"Gold - Fact\", GOLD_FACT_TABLE),\n",
    "        (\"Gold - Customer Dim\", GOLD_DIM_CUSTOMER),\n",
    "        (\"Gold - Product Dim\", GOLD_DIM_PRODUCT),\n",
    "        (\"Gold - Seller Dim\", GOLD_DIM_SELLER),\n",
    "        (\"Gold - Date Dim\", GOLD_DIM_DATE)\n",
    "    ]\n",
    "    \n",
    "    for layer, table_name in tables_info:\n",
    "        try:\n",
    "            count = spark.table(table_name).count()\n",
    "            print(f\"‚úÖ {layer:15} | {table_name:20} | {count:8,} records\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {layer:15} | {table_name:20} | Error: {str(e)}\")\n",
    "    \n",
    "    # Data quality summary\n",
    "    print(f\"\\nüîç DATA QUALITY METRICS:\")\n",
    "    df_silver = spark.table(SILVER_TABLE)\n",
    "    \n",
    "    total_records = df_silver.count()\n",
    "    successful_orders = df_silver.filter(col(\"order_status\").isin([\"DELIVERED\", \"SHIPPED\"])).count()\n",
    "    failed_orders = df_silver.filter(col(\"order_status\").isin([\"CANCELLED\", \"RETURNED\"])).count()\n",
    "    suspicious_records = df_silver.filter(col(\"is_suspicious_amount\") == 1).count()\n",
    "    \n",
    "    success_rate = (successful_orders / total_records) * 100\n",
    "    failure_rate = (failed_orders / total_records) * 100\n",
    "    \n",
    "    print(f\"Total Records Processed: {total_records:,}\")\n",
    "    print(f\"Successful Orders: {successful_orders:,} ({success_rate:.1f}%)\")\n",
    "    print(f\"Failed Orders: {failed_orders:,} ({failure_rate:.1f}%)\")\n",
    "    print(f\"Suspicious Records: {suspicious_records:,}\")\n",
    "    \n",
    "    # Business metrics\n",
    "    print(f\"\\nüí∞ BUSINESS METRICS:\")\n",
    "    df_metrics = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            ROUND(SUM(total_amount), 2) as total_revenue,\n",
    "            ROUND(AVG(total_amount), 2) as avg_order_value,\n",
    "            COUNT(DISTINCT customer_id) as unique_customers,\n",
    "            COUNT(DISTINCT product_id) as unique_products,\n",
    "            MIN(order_date) as earliest_order,\n",
    "            MAX(order_date) as latest_order\n",
    "        FROM sales_silver\n",
    "        WHERE order_status IN ('DELIVERED', 'SHIPPED')\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"Total Revenue: ${df_metrics['total_revenue']:,.2f}\")\n",
    "    print(f\"Average Order Value: ${df_metrics['avg_order_value']:,.2f}\")\n",
    "    print(f\"Unique Customers: {df_metrics['unique_customers']:,}\")\n",
    "    print(f\"Unique Products: {df_metrics['unique_products']:,}\")\n",
    "    print(f\"Date Range: {df_metrics['earliest_order']} to {df_metrics['latest_order']}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ PIPELINE STATUS: COMPLETED SUCCESSFULLY\")\n",
    "    print(f\"üïê Execution completed at: {datetime.now()}\")\n",
    "\n",
    "# Generate pipeline summary\n",
    "generate_pipeline_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "829d241d-99c9-4442-a7f6-41692914ef73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìù 8. Autoavalia√ß√£o\n",
    "\n",
    "### 8.1 Objetivos Atingidos ‚úÖ\n",
    "\n",
    "**Objetivos Principais Alcan√ßados:**\n",
    "\n",
    "1. **‚úÖ Coleta e Modelagem de Dados:**\n",
    "   - Implementa√ß√£o completa da Arquitetura Medalh√£o (Bronze, Silver, Gold)\n",
    "   - Cria√ß√£o de modelo dimensional (Star Schema) com tabelas fato e dimens√£o\n",
    "   - Documenta√ß√£o abrangente do cat√°logo de dados e linhagem\n",
    "\n",
    "2. **‚úÖ Pipeline ETL Funcional:**\n",
    "   - Pipeline automatizado de extra√ß√£o, transforma√ß√£o e carga\n",
    "   - Valida√ß√µes de qualidade de dados em cada camada\n",
    "   - Rastreamento de metadados e versionamento\n",
    "\n",
    "3. **‚úÖ An√°lises de Neg√≥cio:**\n",
    "   - Respostas para todas as 5 perguntas de pesquisa definidas\n",
    "   - Insights sobre performance de vendas, comportamento de clientes e produtos\n",
    "   - Visualiza√ß√µes interativas para dashboards\n",
    "\n",
    "4. **‚úÖ Qualidade de Dados:**\n",
    "   - An√°lise detalhada de completude, consist√™ncia e validade\n",
    "   - Implementa√ß√£o de regras de neg√≥cio e flags de qualidade\n",
    "   - Tratamento de valores suspeitos e inconsist√™ncias\n",
    "\n",
    "### 8.2 Perguntas de Pesquisa - Status de Resolu√ß√£o\n",
    "\n",
    "| Pergunta | Status | Resultado/Insight Principal |\n",
    "|----------|--------|------------------------------|\n",
    "| **Volume de vendas por per√≠odo** | ‚úÖ Respondida | Identificada sazonalidade e padr√µes mensais |\n",
    "| **Produtos top performance** | ‚úÖ Respondida | Ranking por receita e quantidade vendida |\n",
    "| **Impacto de descontos** | ‚úÖ Respondida | Correla√ß√£o entre desconto e volume de vendas |\n",
    "| **An√°lise geogr√°fica** | ‚úÖ Respondida | EUA como principal mercado, seguido por Canad√° |\n",
    "| **M√©todos de pagamento** | ‚úÖ Respondida | Diversidade de m√©todos com prefer√™ncias regionais |\n",
    "\n",
    "### 8.3 Dificuldades Encontradas üöß\n",
    "\n",
    "1. **Limita√ß√µes do Dataset:**\n",
    "   - Dataset simulado com apenas 173 registros\n",
    "   - Aus√™ncia de dados hist√≥ricos mais extensos para an√°lise temporal robusta\n",
    "   - Algumas inconsist√™ncias naturais em dados sint√©ticos\n",
    "\n",
    "2. **Complexidade da Arquitetura:**\n",
    "   - Balanceamento entre simplicidade e completude do modelo dimensional\n",
    "   - Escolha de agrega√ß√µes apropriadas para as tabelas Gold\n",
    "   - Defini√ß√£o de regras de qualidade sem requisitos de neg√≥cio reais\n",
    "\n",
    "3. **Ambiente Databricks:**\n",
    "   - Configura√ß√£o de caminhos de arquivo para diferentes ambientes\n",
    "   - Otimiza√ß√£o de performance para datasets maiores\n",
    "   - Integra√ß√£o com ferramentas de orquestra√ß√£o\n",
    "\n",
    "### 8.4 Trabalhos Futuros üöÄ\n",
    "\n",
    "**Melhorias T√©cnicas:**\n",
    "1. **Automa√ß√£o Avan√ßada:**\n",
    "   - Implementar Databricks Workflows para orquestra√ß√£o completa\n",
    "   - Adicionar testes automatizados de qualidade de dados\n",
    "   - Configurar alertas de monitoramento e falhas\n",
    "\n",
    "2. **Escalabilidade:**\n",
    "   - Otimizar particionamento das tabelas Delta\n",
    "   - Implementar estrat√©gias de caching para consultas frequentes\n",
    "   - Adicionar processamento incremental (CDC - Change Data Capture)\n",
    "\n",
    "3. **Governan√ßa de Dados:**\n",
    "   - Implementar Unity Catalog para governan√ßa centralizada\n",
    "   - Adicionar controles de acesso baseados em fun√ß√£o\n",
    "   - Configurar auditoria e logging detalhado\n",
    "\n",
    "**Expans√µes de Neg√≥cio:**\n",
    "1. **Analytics Avan√ßados:**\n",
    "   - Modelos de machine learning para previs√£o de vendas\n",
    "   - An√°lise de cesta de mercado (market basket analysis)\n",
    "   - Segmenta√ß√£o avan√ßada de clientes com clustering\n",
    "\n",
    "2. **Visualiza√ß√µes Interativas:**\n",
    "   - Dashboard em tempo real com Power BI ou Tableau\n",
    "   - Relat√≥rios executivos automatizados\n",
    "   - Alertas de KPIs cr√≠ticos\n",
    "\n",
    "3. **Integra√ß√£o de Dados:**\n",
    "   - Conectar com mais fontes de dados (redes sociais, reviews)\n",
    "   - Implementar API para integra√ß√£o com sistemas transacionais\n",
    "   - Adicionar dados externos (economia, sazonalidade)\n",
    "\n",
    "### 8.5 Conclus√£o üéØ\n",
    "\n",
    "Este MVP demonstrou com sucesso a implementa√ß√£o de um pipeline de dados moderno utilizando a Arquitetura Medalh√£o no Databricks. Apesar das limita√ß√µes do dataset, conseguimos:\n",
    "\n",
    "- **Estruturar** um pipeline robusto e escal√°vel\n",
    "- **Garantir** qualidade dos dados atrav√©s de m√∫ltiplas camadas de valida√ß√£o  \n",
    "- **Responder** a todas as perguntas de neg√≥cio propostas\n",
    "- **Gerar** insights acion√°veis para tomada de decis√£o\n",
    "- **Documentar** todo o processo para reprodutibilidade\n",
    "\n",
    "O projeto serve como uma **base s√≥lida** para implementa√ß√µes em produ√ß√£o e pode ser facilmente adaptado para diferentes dom√≠nios de neg√≥cio e volumes de dados maiores.\n",
    "\n",
    "**Valor Agregado:** Este MVP demonstra compet√™ncia t√©cnica em engenharia de dados moderna, desde a coleta at√© a an√°lise, seguindo as melhores pr√°ticas da ind√∫stria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e251236-37e7-473d-99db-d812b2ba7a93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Validate Silver layer data quality\n",
    "SELECT \n",
    "    'SILVER_VALIDATION' as layer,\n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(DISTINCT order_id) as unique_orders,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers,\n",
    "    COUNT(DISTINCT product_id) as unique_products,\n",
    "    MIN(order_date) as earliest_order,\n",
    "    MAX(order_date) as latest_order,\n",
    "    AVG(total_amount) as avg_order_value,\n",
    "    SUM(CASE WHEN is_suspicious_amount = 1 THEN 1 ELSE 0 END) as suspicious_records,\n",
    "    SUM(CASE WHEN is_failed_order = 1 THEN 1 ELSE 0 END) as failed_orders\n",
    "FROM sales_silver;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90ad2642-9af0-403a-af4d-7d65ab9d121d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- PERGUNTA 4: An√°lise por Pa√≠s/Regi√£o\n",
    "SELECT \n",
    "    c.country,\n",
    "    c.state,\n",
    "    COUNT(DISTINCT c.customer_id) as unique_customers,\n",
    "    COUNT(f.order_key) as total_orders,\n",
    "    ROUND(SUM(f.total_amount), 2) as total_revenue,\n",
    "    ROUND(AVG(f.total_amount), 2) as avg_order_value,\n",
    "    ROUND(SUM(f.total_amount) / COUNT(DISTINCT c.customer_id), 2) as revenue_per_customer\n",
    "FROM dim_customer c\n",
    "JOIN fact_sales f ON c.customer_id = f.customer_key\n",
    "WHERE f.order_status NOT IN ('CANCELLED', 'RETURNED')\n",
    "GROUP BY c.country, c.state\n",
    "ORDER BY total_revenue DESC;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6479701655613846,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Amazon_Sales_Analysis_MVP",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
